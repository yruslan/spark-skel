
hadoop.redacted.tokens = [password, secret, session.token]

hadoop.option {
  # Authentication providerc can be
  #  - org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider
  #  - com.amazonaws.auth.profile.ProfileCredentialsProvider
  #  - com.amazonaws.auth.InstanceProfileCredentialsProvider
  # Use the default provider chain. It will use the first authentication provider that succeeds
  fs.s3a.aws.credentials.provider = "com.amazonaws.auth.DefaultAWSCredentialsProviderChain"

  # When an AWS profile is used from ~/.aws, you can set
  # AWS_PROFILE to specify the exact profile and
  # AWS_CREDENTIAL_PROFILES_FILE (and AWS_CONFIG_FILE) to specify the .aws config file if it is not ~/.aws/credentials

  # AWS credentials
  #fs.s3a.access.key = "AAABBBAAABBBAAABBBAA111")
  #fs.s3a.secret.key = "abc123abc123abc123abc123abc123abc123"
  # The session token for temporary credentials spec
  #fs.s3a.session.token = ""

  # Explicitly specify the endpoint
  #fs.s3a.endpoint = "s3.af-south-1.amazonaws.com"

  # Enable bucket key encruption
  fs.s3a.server-side-encryption-bucket-key-enabled = "true"

  # Enable magic committer for all buckets to have the best tradeoff between performance and safety
  fs.s3a.committer.name = "magic"
  fs.s3a.committer.magic.enabled = "true"

  # Make sure the default committer (file committer) is never user, since the maximum version is 2
  mapreduce.fileoutputcommitter.algorithm.version = 10

  # Specify the KSM key for server-side encryption
  #fs.s3a.server-side-encryption.key = "arn:aws:kms:***"
  #fs.s3a.server-side-encryption-algorithm = "SSE-KMS"
}

# Spark configuration
spark.conf.option {
  # Run Spark in local master mode
  spark.master = "local[*]"

  # Enable magic committer in Sark
  spark.sql.sources.commitProtocolClass = "org.apache.spark.internal.io.cloud.PathOutputCommitProtocol"
  spark.sql.parquet.output.committer.class = "org.apache.spark.internal.io.cloud.BindingParquetOutputCommitter"
}
